import os
import json
from openai import OpenAI
from dotenv import load_dotenv
from cache_manager import (
    get_content_hash, 
    get_cached_result, 
    save_cached_result,
    clear_expired_cache
)

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# å•Ÿå‹•æ™‚æ¸…é™¤éæœŸå¿«å–
clear_expired_cache()

def analyze_article(text, title="", source="", published_at=""):
    """
    åˆ†æå–®ç¯‡æ–‡ç« ï¼Œå›å‚³å„ªåŒ–ç‰ˆ JSON çµæ§‹ï¼ˆå¸¶è­‰æ“šã€å¯æ±ºç­–ï¼‰
    ä½¿ç”¨å¿«å–é¿å…é‡è¤‡åˆ†æç›¸åŒå…§å®¹
    
    Args:
        text: æ–°èå…§å®¹
        title: æ–°èæ¨™é¡Œï¼ˆå¯é¸ï¼‰
        source: æ–°èä¾†æºï¼ˆå¯é¸ï¼‰
        published_at: ç™¼å¸ƒæ™‚é–“ ISO 8601ï¼ˆå¯é¸ï¼‰
    
    Returns:
        JSON çµæ§‹åŒ…å«ï¼šsummary, highlights, category, entities, sentiment, 
        time_horizon, impact_scope, why_it_matters, insight, 
        action_suggestions, confidence, uncertainties
    """
    if not text: 
        return None
    
    # æª¢æŸ¥å¿«å–
    metadata = {'title': title, 'source': source}
    cache_key = get_content_hash(text, metadata)
    cached_result = get_cached_result(cache_key, cache_type="article")
    
    if cached_result:
        return cached_result
    
    # é¿å… Token çˆ†é‡ï¼Œæˆªå–å‰ 2500 å­—
    input_text = text[:2500]
    
    system_prompt = """# Role
ä½ æ˜¯ä¸€ä½ã€Œç¹é«”ä¸­æ–‡ã€çš„è³‡æ·±ç§‘æŠ€/åŠ å¯†ç”¢æ¥­åˆ†æå¸«ï¼Œæ“…é•·å°‡æ–°èè½‰æˆå¯åŸ·è¡Œçš„æ±ºç­–è³‡è¨Šã€‚

# Core Rules (Very Important)
1) åƒ…èƒ½æ ¹æ“šã€æ–°èå…§å®¹ã€‘æ¨è«–ï¼Œä¸å¾—æœæ’¬ã€ä¸å¾—ç·¨é€ ä¸å­˜åœ¨çš„æ•¸å­—/äººå/äº‹ä»¶ã€‚
2) å¦‚å…§å®¹ä¸è¶³ä»¥æ”¯æŒçµè«–ï¼Œè«‹æ˜ç¢ºå¯«ã€Œè³‡è¨Šä¸è¶³ã€ä¸¦é™ä½ confidenceã€‚
3) æ‰€æœ‰é‡é»å¿…é ˆé™„ä¸Š evidenceï¼šç”¨ã€ŒåŸæ–‡ç‰‡æ®µã€æˆ–ã€Œé—œéµå¥ã€(<=25å­—) ä½œç‚ºä½è­‰ã€‚
4) è¼¸å‡ºå¿…é ˆæ˜¯ã€Œå–®ä¸€ JSONã€ï¼Œä¸å¯åŒ…å« Markdownã€ä¸å¯åŒ…å«å¤šé¤˜æ–‡å­—ã€ä¸å¯åŠ è¨»è§£ã€‚
5) åˆ†é¡è«‹ä½¿ç”¨æ—¢å®šé¡åˆ¥ï¼Œä¸ç¬¦åˆæ‰è‡ªè¨‚ï¼šäººå·¥æ™ºæ…§ã€å€å¡Šéˆã€ç¡¬é«”ã€å•†æ¥­ã€è³‡å®‰ã€æ”¿ç­–æ³•è¦ã€å®è§€ç¶“æ¿Ÿã€å…¶ä»–ã€‚

# Task
é–±è®€ã€æ–°èå…§å®¹ã€‘ï¼Œå›å‚³ä¸‹åˆ— JSONï¼ˆæ¬„ä½ä¸å¯ç¼ºæ¼ï¼‰ï¼š
{
  "title": "è‹¥åŸæ–‡å«æ¨™é¡Œå‰‡å¡«ï¼Œå¦å‰‡ç©ºå­—ä¸²",
  "source": "è‹¥åŸæ–‡å«ä¾†æº/åª’é«”å‰‡å¡«ï¼Œå¦å‰‡ç©ºå­—ä¸²",
  "published_at": "è‹¥åŸæ–‡å«æ—¥æœŸæ™‚é–“(ISO 8601å„ªå…ˆ)å‰‡å¡«ï¼Œå¦å‰‡ç©ºå­—ä¸²",
  "language": "zh/ja/en/otherï¼ˆä¾å…§å®¹åˆ¤æ–·ï¼‰",

  "summary": "æ•˜è¿°æ€§æ‘˜è¦(ç¹é«”ä¸­æ–‡ï¼Œ80-120å­—ï¼Œå«ä¸»è©/äº‹ä»¶/å½±éŸ¿)",
  "highlights": [
    {"point": "é‡é»1(<=26å­—)", "evidence": "ä½è­‰ç‰‡æ®µ(<=25å­—)"},
    {"point": "é‡é»2(<=26å­—)", "evidence": "ä½è­‰ç‰‡æ®µ(<=25å­—)"},
    {"point": "é‡é»3(<=26å­—)", "evidence": "ä½è­‰ç‰‡æ®µ(<=25å­—)"}
  ],

  "category": "åˆ†é¡",
  "tags": ["3-8å€‹æ¨™ç±¤(ç¹ä¸­ï¼Œé¿å…é‡è¤‡)"],

  "entities": {
    "companies": ["å…¬å¸/çµ„ç¹”"],
    "people": ["äººç‰©"],
    "products": ["ç”¢å“/å”è­°/æ¨¡å‹/éˆ"],
    "tickers": ["BTC","ETH","SOL"... è‹¥ç„¡å‰‡[]],
    "locations": ["åœ°é»/åœ‹å®¶å€åŸŸ"]
  },

  "sentiment": "bullish/bearish/neutral/mixedï¼ˆé‡å°è©²æ–°èå°ç”¢æ¥­æˆ–å¸‚å ´çš„å‚¾å‘ï¼‰",
  "time_horizon": "now/1w/1m/1q/1yï¼ˆä¸»è¦å½±éŸ¿æ™‚é–“å°ºåº¦ï¼‰",
  "impact_scope": ["æŠ•è³‡å¸‚å ´","é–‹ç™¼è€…","ä¼æ¥­æ¡ç”¨","ç›£ç®¡","å®‰å…¨é¢¨éšª","ä¾›æ‡‰éˆ","å…¶ä»–"],
  "why_it_matters": "ä¸€å¥è©±ï¼šé€™å‰‡æ–°èå°æ±ºç­–è€…/æŠ•è³‡è€…/é–‹ç™¼è€…çš„é‡è¦æ€§(<=40å­—)",

  "insight": "ä¸€å¥è©±ç”¢æ¥­æ´å¯Ÿï¼ˆåçµæ§‹æ€§ã€é¿å…ç©ºæ³›ï¼‰",

  "action_suggestions": [
    "çµ¦ç”¢å“/å·¥ç¨‹/æŠ•è³‡/ç‡Ÿé‹å¯åšçš„ä¸‹ä¸€æ­¥ï¼ˆæœ€å¤š3æ¢ã€æ¯æ¢<=18å­—ã€å¯ç‚º'å…ˆè§€å¯Ÿ'ï¼‰"
  ],

  "confidence": 0.0,
  "uncertainties": ["ä¸ç¢ºå®šé»/ç¼ºå°‘è³‡è¨Šï¼ˆæœ€å¤š3æ¢ï¼‰"]
}

# Confidence Scoring Guide
- 0.8~1.0ï¼šè³‡è¨Šå®Œæ•´ä¸”æœ‰æ˜ç¢ºä½è­‰
- 0.5~0.7ï¼šé—œéµè³‡è¨Šé½Šä½†ä»æœ‰ç¼ºå£
- 0.2~0.4ï¼šè³‡è¨Šç‰‡æ®µåŒ–ï¼Œåªèƒ½åšä¿å®ˆæ‘˜è¦
- 0.0~0.1ï¼šå…§å®¹ä¸æ§‹æˆæœ‰æ•ˆæ–°è/å¤ªçŸ­/ç„¡æ³•åˆ¤æ–·"""
    
    # æ§‹å»ºç”¨æˆ¶è¼¸å…¥ï¼ŒåŒ…å«å¯ç”¨çš„ metadata
    user_content = input_text
    if title or source or published_at:
        metadata_parts = []
        if title:
            metadata_parts.append(f"æ¨™é¡Œï¼š{title}")
        if source:
            metadata_parts.append(f"ä¾†æºï¼š{source}")
        if published_at:
            metadata_parts.append(f"ç™¼å¸ƒæ™‚é–“ï¼š{published_at}")
        user_content = "\n".join(metadata_parts) + "\n\n" + input_text
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.25,  # å„ªåŒ–ï¼š0.2~0.3ï¼Œä½¿ç”¨ 0.25
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        
        # å„²å­˜åˆ°å¿«å–
        save_cached_result(
            cache_key, 
            result, 
            cache_type="article",
            metadata={'title': title, 'source': source, 'published_at': published_at}
        )
        
        return result
    except Exception as e:
        print(f"âŒ AI åˆ†æå¤±æ•—: {e}")
        return None

def analyze_category_group(category_name, articles_in_category):
    """
    åˆ†æåŒä¸€åˆ†é¡çš„å¤šç¯‡æ–‡ç« ï¼Œç”Ÿæˆçµ±åˆåˆ†æï¼ˆå„ªåŒ–ç‰ˆï¼šå»é‡ã€æŠ½ä¸»ç·šã€ç”¢å‡ºå¯è¿½è¹¤ä¿¡è™Ÿï¼‰
    ä½¿ç”¨å¿«å–é¿å…é‡è¤‡åˆ†æç›¸åŒåˆ†é¡çµ„åˆ
    
    Args:
        category_name: åˆ†é¡åç¨±
        articles_in_category: è©²åˆ†é¡ä¸‹çš„æ–‡ç« åˆ—è¡¨ï¼ˆæ‡‰åŒ…å« analyze_article çš„å®Œæ•´çµæœï¼‰
    
    Returns:
        JSON çµæ§‹åŒ…å«ï¼šexecutive_summary, storylines, key_points, trend_analysis,
        risks, opportunities, signals_to_watch, confidence, uncertainties
    """
    if not articles_in_category:
        return None
    
    # æª¢æŸ¥å¿«å–ï¼ˆåŸºæ–¼åˆ†é¡åç¨±å’Œæ–‡ç« æ¨™é¡Œçµ„åˆï¼‰
    article_titles = [art.get('title', '') for art in articles_in_category]
    cache_content = f"{category_name}:{','.join(sorted(article_titles))}"
    cache_key = get_content_hash(cache_content)
    cached_result = get_cached_result(cache_key, cache_type="category")
    
    if cached_result:
        # æ›´æ–° article_count å’Œ article_titlesï¼ˆå¯èƒ½ä¸åŒï¼‰
        cached_result['article_count'] = len(articles_in_category)
        cached_result['article_titles'] = article_titles
        return cached_result
    
    # æº–å‚™æ–‡ç« è³‡æ–™ï¼ˆä½¿ç”¨çµæ§‹åŒ–çµæœè€ŒéåŸå§‹å…§å®¹ï¼‰
    article_data = []
    article_titles = []
    
    for article in articles_in_category:
        title = article.get('title', 'ç„¡æ¨™é¡Œ')
        article_titles.append(title)
        
        # å„ªå…ˆä½¿ç”¨çµæ§‹åŒ–åˆ†æçµæœ
        summary = article.get('summary', '')
        highlights = article.get('highlights', [])
        insight = article.get('insight', '')
        entities = article.get('entities', {})
        sentiment = article.get('sentiment', 'neutral')
        why_it_matters = article.get('why_it_matters', '')
        
        # å¦‚æœæ²’æœ‰çµæ§‹åŒ–çµæœï¼Œä½¿ç”¨åŸå§‹å…§å®¹
        if not summary:
            summary = article.get('content', '')[:500]
        
        article_data.append({
            'title': title,
            'summary': summary,
            'highlights': highlights,
            'insight': insight,
            'entities': entities,
            'sentiment': sentiment,
            'why_it_matters': why_it_matters
        })
    
    # æ§‹å»ºè¼¸å…¥å…§å®¹ï¼ˆä½¿ç”¨çµæ§‹åŒ–è³‡æ–™ï¼Œæ›´çœ tokenï¼Œé€²ä¸€æ­¥å„ªåŒ–ï¼‰
    combined_input = ""
    for art in article_data:
        combined_input += f"\nã€{art['title']}ã€‘\n"
        # é™åˆ¶æ‘˜è¦é•·åº¦ä»¥ç¯€çœ token
        summary = art['summary'][:200] if len(art['summary']) > 200 else art['summary']
        combined_input += f"æ‘˜è¦ï¼š{summary}\n"
        
        # åªå–å‰ 2 å€‹é‡é»
        if art.get('highlights'):
            if isinstance(art['highlights'], list) and len(art['highlights']) > 0:
                if isinstance(art['highlights'][0], dict):
                    highlights_str = "; ".join([h.get('point', '') for h in art['highlights'][:2]])
                else:
                    highlights_str = "; ".join(art['highlights'][:2])
                combined_input += f"é‡é»ï¼š{highlights_str}\n"
        
        # åªå‚³éé—œéµè³‡è¨Š
        combined_input += f"æ´å¯Ÿï¼š{art.get('insight', '')[:80]}\n"  # é™åˆ¶æ´å¯Ÿé•·åº¦
        combined_input += f"é‡è¦æ€§ï¼š{art.get('why_it_matters', '')}\n"
        combined_input += f"æƒ…ç·’ï¼š{art.get('sentiment', 'neutral')}\n"
    
    # é™åˆ¶ç¸½é•·åº¦
    if len(combined_input) > 8000:
        combined_input = combined_input[:8000] + "..."
    
    system_prompt = """ä½ æ˜¯ä¸€ä½ç¹é«”ä¸­æ–‡çš„è³‡æ·±ç§‘æŠ€/åŠ å¯†ç”¢æ¥­åˆ†æå¸«ï¼Œæ“…é•·æŠŠå¤šç¯‡æ–°èã€Œå»é‡ã€æŠ“ä¸»ç·šã€åšæƒ…å¢ƒæ¨æ¼”ã€ã€‚

# Rules
1) åƒ…èƒ½ä½¿ç”¨è¼¸å…¥æ–‡ç« å…§å®¹ï¼›ä¸å¯å¼•å…¥å¤–éƒ¨èƒŒæ™¯çŸ¥è­˜ç•¶ä½œäº‹å¯¦ã€‚
2) éœ€è¦ã€Œå»é‡ã€ï¼šç›¸åŒäº‹ä»¶è«‹åˆä½µæè¿°ï¼Œä¸è¦é‡è¤‡åˆ—é»ã€‚
3) è‹¥åŒåˆ†é¡å…§å‡ºç¾çŸ›ç›¾èªªæ³•ï¼Œå¿…é ˆæŒ‡å‡ºä¸¦æ¨™è¨˜ uncertaintiesã€‚
4) æ¯å€‹ key_point èˆ‡ signal å¿…é ˆèƒ½è¿½æº¯åˆ°è‡³å°‘ä¸€ç¯‡æ–‡ç« æ¨™é¡Œï¼ˆç”¨ titles å¼•ç”¨å³å¯ï¼Œä¸è¦è²¼é•·æ–‡ï¼‰ã€‚
5) è¼¸å‡ºå¿…é ˆæ˜¯å–®ä¸€ JSONï¼Œä¸è¦ Markdownã€‚

# Task
åˆ†æåŒä¸€åˆ†é¡çš„å¤šç¯‡æ–‡ç« ï¼Œå›å‚³ï¼š
{
  "category": "åˆ†é¡åç¨±ï¼ˆèˆ‡è¼¸å…¥ç›¸åŒï¼‰",
  "executive_summary": "çµ±åˆæ‘˜è¦ï¼ˆ200-280å­—ï¼Œç¹é«”ä¸­æ–‡ï¼Œå…ˆçµè«–å¾Œç†ç”±ï¼‰",

  "storylines": [
    {
      "theme": "ä¸»ç·šæ¨™é¡Œ(<=18å­—)",
      "what_happened": "ç™¼ç”Ÿä»€éº¼(<=60å­—)",
      "why_it_matters": "é‡è¦æ€§(<=60å­—)",
      "who_is_affected": ["æŠ•è³‡è€…","é–‹ç™¼è€…","äº¤æ˜“æ‰€","ä¼æ¥­","ç›£ç®¡æ–¹","ç”¨æˆ¶","å…¶ä»–"],
      "time_horizon": "now/1w/1m/1q/1y",
      "titles": ["æ”¯æ’æ­¤ä¸»ç·šçš„æ–‡ç« æ¨™é¡Œ1","æ¨™é¡Œ2"]
    }
  ],

  "key_points": [
    {"point":"é‡é»(<=26å­—)","titles":["æ¨™é¡Œ1"]},
    {"point":"é‡é»(<=26å­—)","titles":["æ¨™é¡Œ2"]}
  ],

  "trend_analysis": "è¶¨å‹¢åˆ†æèˆ‡ç”¢æ¥­å½±éŸ¿ï¼ˆ150-220å­—ï¼Œç¹é«”ä¸­æ–‡ï¼›é¿å…å£è™Ÿï¼‰",

  "risks": [
    {"risk":"é¢¨éšª(<=22å­—)","type":"regulatory/security/market/tech/ops/other","titles":["æ¨™é¡Œ1"]}
  ],

  "opportunities": [
    {"opp":"æ©Ÿæœƒ(<=22å­—)","titles":["æ¨™é¡Œ1"]}
  ],

  "signals_to_watch": [
    {"signal":"å¯è§€æ¸¬ä¿¡è™Ÿ(<=22å­—)","how_to_track":"ç”¨ä»€éº¼æŒ‡æ¨™/äº‹ä»¶è¿½(<=24å­—)","expected_direction":"up/down/unknown"}
  ],

  "confidence": 0.0,
  "uncertainties": ["è³‡æ–™ç¼ºå£/çŸ›ç›¾é»ï¼ˆæœ€å¤š4æ¢ï¼‰"]
}"""
    
    user_content = f"""åˆ†é¡ï¼š{category_name}
æ–‡ç« æ•¸é‡ï¼š{len(articles_in_category)}
æ–‡ç« æ¨™é¡Œï¼š{', '.join(article_titles[:10])}

æ–‡ç« å…§å®¹ï¼š
{combined_input}
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.35,  # å„ªåŒ–ï¼š0.3~0.4ï¼Œä½¿ç”¨ 0.35
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        result['article_count'] = len(articles_in_category)
        result['article_titles'] = article_titles
        
        # å„²å­˜åˆ°å¿«å–
        save_cached_result(
            cache_key,
            result,
            cache_type="category",
            metadata={'category': category_name, 'article_count': len(articles_in_category)}
        )
        
        return result
    except Exception as e:
        print(f"âŒ åˆ†é¡åˆ†æå¤±æ•— ({category_name}): {e}")
        return None

def generate_daily_briefing(category_analyses, source_articles=None):
    """
    æ ¹æ“šåˆ†é¡åˆ†æçµæœç”Ÿæˆæ¯æ—¥æ±ºç­–æ—¥å ±ï¼ˆå„ªåŒ–ç‰ˆï¼šå»é™¤ AI æ„Ÿã€æ›´è‡ªç„¶æµæš¢ï¼‰
    
    Args:
        category_analyses: åˆ†é¡åˆ†æçµæœåˆ—è¡¨ï¼ˆæ‡‰åŒ…å« analyze_category_group çš„å®Œæ•´çµæœï¼‰
        source_articles: åŸå§‹æ–‡ç« åˆ—è¡¨ï¼ˆåŒ…å« URLï¼‰ï¼Œç”¨æ–¼ç”Ÿæˆä¾†æºé€£çµ
    
    Returns:
        Markdown æ ¼å¼çš„æ±ºç­–æ—¥å ±
    """
    if not category_analyses:
        return "âš ï¸ è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•ç”Ÿæˆæ—¥å ±ã€‚"

    # æ•´ç†è³‡æ–™çµ¦ AIï¼ˆä½¿ç”¨çµæ§‹åŒ–çµæœï¼‰
    input_text = ""
    article_urls_by_category = {}
    
    for cat_analysis in category_analyses:
        category = cat_analysis.get('category', 'æœªåˆ†é¡')
        executive_summary = cat_analysis.get('executive_summary', cat_analysis.get('summary', ''))
        storylines = cat_analysis.get('storylines', [])
        key_points = cat_analysis.get('key_points', [])
        trend_analysis = cat_analysis.get('trend_analysis', '')
        risks = cat_analysis.get('risks', [])
        opportunities = cat_analysis.get('opportunities', [])
        signals_to_watch = cat_analysis.get('signals_to_watch', [])
        confidence = cat_analysis.get('confidence', 0.0)
        uncertainties = cat_analysis.get('uncertainties', [])
        article_count = cat_analysis.get('article_count', 0)
        article_titles = cat_analysis.get('article_titles', [])
        
        # æ”¶é›†è©²åˆ†é¡çš„æ–‡ç«  URL
        if source_articles:
            article_urls_by_category[category] = []
            for article in source_articles:
                if article.get('category') == category or article.get('title') in article_titles:
                    article_urls_by_category[category].append({
                        'title': article.get('title', ''),
                        'url': article.get('url', ''),
                        'source': article.get('source', '')
                    })
        
        input_text += f"\n## {category} ({article_count}ç¯‡)\n"
        input_text += f"æ‘˜è¦ï¼š{executive_summary}\n"
        
        if storylines:
            input_text += f"ä¸»ç·šï¼š{len(storylines)}æ¢\n"
            for sl in storylines[:2]:  # æœ€å¤šé¡¯ç¤º2æ¢ä¸»ç·š
                input_text += f"  - {sl.get('theme', '')}: {sl.get('what_happened', '')}\n"
        
        if key_points:
            if isinstance(key_points[0], dict):
                points_str = "; ".join([kp.get('point', '') for kp in key_points[:3]])
            else:
                points_str = "; ".join(key_points[:3])
            input_text += f"é‡é»ï¼š{points_str}\n"
        
        input_text += f"è¶¨å‹¢ï¼š{trend_analysis}\n"
        
        if risks:
            risks_str = "; ".join([r.get('risk', '') if isinstance(r, dict) else r for r in risks[:2]])
            input_text += f"é¢¨éšªï¼š{risks_str}\n"
        
        if opportunities:
            opps_str = "; ".join([o.get('opp', '') if isinstance(o, dict) else o for o in opportunities[:2]])
            input_text += f"æ©Ÿæœƒï¼š{opps_str}\n"
        
        if signals_to_watch:
            signals_str = "; ".join([s.get('signal', '') if isinstance(s, dict) else s for s in signals_to_watch[:3]])
            input_text += f"ä¿¡è™Ÿï¼š{signals_str}\n"
        
        input_text += f"ä¿¡å¿ƒåº¦ï¼š{confidence}\n"
        if uncertainties:
            input_text += f"ä¸ç¢ºå®šæ€§ï¼š{'; '.join(uncertainties[:2])}\n"
    
    if len(input_text) > 6000: 
        input_text = input_text[:6000] + "..."

    system_prompt = """ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œçš„ç§‘æŠ€èˆ‡åŠ å¯†ç”¢æ¥­ç·¨è¼¯ï¼Œæ“…é•·å°‡åˆ†æå ±å‘Šè½‰æ›æˆè‡ªç„¶ã€æµæš¢ã€æœ‰ç¯€å¥æ„Ÿçš„ä¸­æ–‡å…§å®¹ã€‚

# æ ¸å¿ƒåŸå‰‡ï¼ˆéå¸¸é‡è¦ï¼‰
1. **ä½¿ç”¨è‡ªç„¶ã€æµæš¢çš„ä¸­æ–‡**ï¼šæ¨¡ä»¿è³‡æ·±ä½œè€…æˆ–è¨˜è€…çš„æ‰‹ç­†ï¼Œé¿å…æ©Ÿæ¢°å¼è¡¨é”
2. **é¿å… AI æ„Ÿè©å½™**ï¼šä¸è¦ä½¿ç”¨ã€Œé¦–å…ˆã€å…¶æ¬¡ã€æœ€å¾Œã€ã€ã€Œç¸½ä¹‹ã€ã€ã€Œç¶œä¸Šæ‰€è¿°ã€ã€ã€Œæ‡‰è©²æ³¨æ„çš„æ˜¯ã€ã€ã€Œå€¼å¾—æ³¨æ„çš„æ˜¯ã€ã€ã€Œæ¯«ç„¡ç–‘å•ã€ã€ã€Œä¸€èˆ¬ä¾†èªªã€ç­‰
3. **ä¸»å‹•èªæ…‹å„ªå…ˆ**ï¼šå°‡è¢«å‹•æ…‹è½‰ç‚ºä¸»å‹•èªæ…‹ï¼Œè®“æ–‡å­—æ›´æœ‰åŠ›é‡
4. **é•·çŸ­å¥äº¤éŒ¯**ï¼šé¿å…é•·å¥å †ç Œï¼Œé©ç•¶ä½¿ç”¨çŸ­å¥å¢åŠ ç¯€å¥æ„Ÿ
5. **å¢åŠ äººæ€§åŒ–ç´°ç¯€**ï¼šåœ¨åˆé©ä½ç½®åŠ å…¥é©åº¦çš„å€‹äººè¦‹è§£ã€å ´æ™¯åŒ–æè¿°æˆ–å…·è±¡æ¯”å–»ï¼ˆé¿å…è€å¥—æ¯”å–»ï¼‰
6. **æ‰“ç ´æ¨¡æ¿çµæ§‹**ï¼šå¦‚æœåŸæ–‡çµæ§‹æ˜¯æ˜é¡¯çš„ã€Œç¸½åˆ†ç¸½ã€æˆ–ã€Œä¸¦åˆ—å¼ã€ï¼Œè«‹èª¿æ•´æ®µè½éŠœæ¥ï¼Œè®“é‚è¼¯è‡ªç„¶éæ¸¡
7. **é©åº¦å†—ä½™**ï¼šåœ¨ä¸å½±éŸ¿æ ¸å¿ƒä¿¡æ¯çš„å‰æä¸‹ï¼Œå¯æ’å…¥å°‘é‡å£èªåŒ–æ’å…¥èªï¼ˆå¦‚ã€Œèªªèµ·ä¾†ã€ã€ã€Œå¯¦éš›ä¸Šã€ï¼‰ã€é™å®šè©ï¼ˆå¦‚ã€Œåœ¨å¤šæ•¸æƒ…æ³ä¸‹ã€ã€ã€Œæˆ–è¨±ã€ï¼‰æˆ–è¼•å¾®çš„èªæ°£è©
8. **é¿å…å®Œç¾ä¸»ç¾©**ï¼šå¯è¼•å¾®èª¿æ•´è«–é»ï¼Œä½¿å…¶çœ‹èµ·ä¾†æ›´åƒå€‹äººè§€é»è€Œéçµ•å°çœŸç†
9. **ä½¿ç”¨å…·é«”ç”Ÿå‹•çš„å‹•è©å’Œåè©**ï¼šæ¸›å°‘æŠ½è±¡è©å½™
10. **å¢åŠ äº’å‹•æ„Ÿ**ï¼šå¯éš¨æ©Ÿå°‡éƒ¨åˆ†é™³è¿°å¥æ”¹ç‚ºåå•å¥æˆ–è¨­å•å¥

# è¼¸å‡ºçµæ§‹ï¼ˆMarkdown - å¿…é ˆåš´æ ¼éµå¾ªæ ¼å¼ï¼‰
è«‹ä½¿ç”¨ä»¥ä¸‹ Markdown æ ¼å¼è¼¸å‡ºï¼Œæ¨™é¡Œå¿…é ˆä½¿ç”¨ # å’Œ ##ï¼š

## ä»Šæ—¥ä¸‰å¥è©±ï¼ˆTL;DRï¼‰
ï¼ˆç”¨è‡ªç„¶çš„å£å»ç¸½çµï¼Œä¸è¦ç”¨ã€Œé¦–å…ˆã€å…¶æ¬¡ã€ï¼Œ**å¿…é ˆä½¿ç”¨åˆ—è¡¨æ ¼å¼**ç›´æ¥åˆ—å‡º 3 å€‹è¦é»ï¼‰
- [è¦é»1ï¼šä¸€å¥è©±æè¿°]
- [è¦é»2ï¼šä¸€å¥è©±æè¿°]
- [è¦é»3ï¼šä¸€å¥è©±æè¿°]

## ğŸ“Š å¸‚å ´æƒ…ç·’å„€è¡¨æ¿
- æƒ…ç·’ï¼š[æƒ…ç·’æè¿°]
- ç†±è©ï¼š[é—œéµè©1ã€é—œéµè©2ã€é—œéµè©3]
- è³‡é‡‘ä¸»å°å› å­ï¼š[å› å­æè¿°]
- ç›£ç®¡ä¸»å°å› å­ï¼š[å› å­æè¿°]
- è³‡å®‰ä¸»å°å› å­ï¼š[å› å­æè¿°]

## ğŸŒŠ æ ¸å¿ƒè¶¨å‹¢åˆ†æ
ï¼ˆ3-5æ¢ storylinesï¼Œæ¯æ¢å«å½±éŸ¿ã€æ™‚é–“å°ºåº¦ã€è¦è¿½çš„ä¿¡è™Ÿï¼‰
- [è¶¨å‹¢1æ¨™é¡Œ]ï¼š[æè¿°]
- [è¶¨å‹¢2æ¨™é¡Œ]ï¼š[æè¿°]
...

## ğŸ§­ æ±ºç­–æŒ‡å¼•

### æŠ•è³‡/äº¤æ˜“
- [å¯åŸ·è¡Œè¦é»1]
- [å¯åŸ·è¡Œè¦é»2]
...

### ç”¢å“/å·¥ç¨‹
- [å¯åŸ·è¡Œè¦é»1]
- [å¯åŸ·è¡Œè¦é»2]
...

### ç‡Ÿé‹/é¢¨æ§
- [å¯åŸ·è¡Œè¦é»1]
- [å¯åŸ·è¡Œè¦é»2]
...

## ğŸ”­ ä»Šæ—¥ç›£æ¸¬æ¸…å–®
- [ ] [ç›£æ¸¬é …ç›®1]
- [ ] [ç›£æ¸¬é …ç›®2]
...

## ğŸ“ˆ åˆ†é¡æ‘˜è¦

### [åˆ†é¡åç¨±1]
- [è¦é»1]
- [è¦é»2]
- [è¦é»3]
ï¼ˆä¿¡å¿ƒåº¦ï¼šX.XXï¼‰

### [åˆ†é¡åç¨±2]
...

## ğŸ§± ä¸ç¢ºå®šæ€§èˆ‡åæ–¹è§€é»
- [ä¸ç¢ºå®šé»1]
- [ä¸ç¢ºå®šé»2]
...

## ğŸ”— è³‡è¨Šä¾†æº
- [æ–‡ç« æ¨™é¡Œ](URL) - *ä¾†æºåç¨±*
- [æ–‡ç« æ¨™é¡Œ](URL) - *ä¾†æºåç¨±*
...

# æ ¼å¼è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰
1. **å¿…é ˆä½¿ç”¨ Markdown æ¨™é¡Œæ ¼å¼**ï¼š
   - ä¸»æ¨™é¡Œä½¿ç”¨ `## `ï¼ˆå…©å€‹äº•è™ŸåŠ ç©ºæ ¼ï¼‰
   - å­æ¨™é¡Œä½¿ç”¨ `### `ï¼ˆä¸‰å€‹äº•è™ŸåŠ ç©ºæ ¼ï¼‰
2. **æ¨™é¡Œå¿…é ˆåŒ…å« emoji å’Œæ–‡å­—**ï¼Œä¾‹å¦‚ï¼š`## ğŸ“Š å¸‚å ´æƒ…ç·’å„€è¡¨æ¿`
3. **åˆ—è¡¨ä½¿ç”¨ `- ` æˆ– `- [ ]`**ï¼ˆç›£æ¸¬æ¸…å–®ç”¨ checkboxï¼‰
   - **ã€Œä»Šæ—¥ä¸‰å¥è©±ã€å€å¡Šå¿…é ˆä½¿ç”¨åˆ—è¡¨æ ¼å¼**ï¼ˆ`- `ï¼‰ï¼Œä¸è¦ä½¿ç”¨æ®µè½
   - **æ‰€æœ‰è¦é»éƒ½æ‡‰è©²ä½¿ç”¨åˆ—è¡¨æ ¼å¼**ï¼Œé™¤éæ˜¯é•·æ®µè½æè¿°
4. **é€£çµä½¿ç”¨ Markdown æ ¼å¼**ï¼š`[æ–‡å­—](URL)`
5. **ç”¨çŸ­å¥ã€çŸ­æ®µè½ã€è¦é»åŒ–**
6. **Bullet ä»¥ã€Œå¯åŸ·è¡Œã€ç‚ºå„ªå…ˆ**ï¼šå‹•è©é–‹é ­ï¼ˆè¿½è¹¤/æš«åœ/è©•ä¼°/å°æ²–/ç¢ºèªâ€¦ï¼‰
7. **é¿å…éæ–¼åˆ¶å¼çš„è¡¨é”ï¼Œè®“æ–‡å­—æœ‰æº«åº¦**
8. **ç›´æ¥è¼¸å‡ºæ½¤è‰²å¾Œçš„å…¨æ–‡ï¼Œç„¡éœ€è§£é‡‹ä¿®æ”¹äº†å“ªè£¡**"""

    # æ·»åŠ ä¾†æºé€£çµè³‡è¨Šåˆ°è¼¸å…¥
    if source_articles and article_urls_by_category:
        input_text += "\n\n## è³‡è¨Šä¾†æº\n"
        for category, urls in article_urls_by_category.items():
            if urls:
                input_text += f"\n### {category}\n"
                for article_info in urls[:5]:  # æ¯å€‹åˆ†é¡æœ€å¤šé¡¯ç¤º5å€‹é€£çµ
                    title = article_info.get('title', '')
                    url = article_info.get('url', '')
                    source = article_info.get('source', '')
                    if url:
                        input_text += f"- [{title}]({url}) - {source}\n"
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ã€åˆ†é¡åˆ†æçµæœã€‘\n{input_text}\n\nè«‹æ ¹æ“šä»¥ä¸Šå…§å®¹ç”Ÿæˆè‡ªç„¶æµæš¢çš„æ—¥å ±ï¼Œä¸¦åœ¨æœ€å¾Œé™„ä¸Šè³‡è¨Šä¾†æºé€£çµã€‚"}
            ],
            temperature=0.7,  # æé«˜æº«åº¦ä»¥å¢åŠ å‰µé€ æ€§å’Œè‡ªç„¶åº¦
        )
        result = response.choices[0].message.content.strip()
        
        # å¾Œè™•ç†ï¼šä¿®æ­£ã€Œä»Šæ—¥ä¸‰å¥è©±ã€æ ¼å¼ï¼ˆå¦‚æœ LLM è¼¸å‡ºæ®µè½è€Œéåˆ—è¡¨ï¼‰
        # æª¢æ¸¬ã€Œä»Šæ—¥ä¸‰å¥è©±ã€å€å¡Šï¼Œå¦‚æœå…§å®¹æ˜¯æ®µè½è€Œéåˆ—è¡¨ï¼Œè‡ªå‹•è½‰æ›
        if "## ä»Šæ—¥ä¸‰å¥è©±" in result or "## ä»Šæ—¥ä¸‰å¥è©±ï¼ˆTL;DRï¼‰" in result:
            import re
            # åŒ¹é…ã€Œä»Šæ—¥ä¸‰å¥è©±ã€å€å¡Šï¼ˆå¾æ¨™é¡Œåˆ°ä¸‹ä¸€å€‹ H2 æˆ–çµå°¾ï¼‰
            pattern = r'(##\s*ä»Šæ—¥ä¸‰å¥è©±[^\n]*\n)(.*?)(?=\n##|\n---|\Z)'
            match = re.search(pattern, result, re.DOTALL)
            if match:
                header = match.group(1)
                content = match.group(2).strip()
                # æª¢æŸ¥æ˜¯å¦ç‚ºåˆ—è¡¨æ ¼å¼ï¼ˆåŒ…å« `- `ï¼‰
                if not content.startswith('-') and '\n- ' not in content:
                    # å°‡æ®µè½è½‰æ›ç‚ºåˆ—è¡¨
                    lines = [line.strip() for line in content.split('\n') if line.strip()]
                    if lines:
                        # éæ¿¾æ‰ç©ºè¡Œå’Œåªæœ‰æ¨™é»çš„è¡Œ
                        valid_lines = [line for line in lines if len(line) > 10 and not line.startswith('ï¼ˆ')]
                        if valid_lines:
                            # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
                            list_content = '\n'.join([f'- {line}' for line in valid_lines[:3]])  # æœ€å¤š3å€‹è¦é»
                            result = result.replace(match.group(0), header + list_content)
        
        # ç¢ºä¿ä¾†æºé€£çµå·²åŒ…å«ï¼ˆå¦‚æœ AI æ²’æœ‰ç”Ÿæˆï¼Œæ‰‹å‹•æ·»åŠ ï¼‰
        if source_articles and article_urls_by_category and "ğŸ”— è³‡è¨Šä¾†æº" not in result and "è³‡è¨Šä¾†æº" not in result:
            result += "\n\n---\n\n## ğŸ”— è³‡è¨Šä¾†æº\n\n"
            for category, urls in article_urls_by_category.items():
                if urls:
                    result += f"### {category}\n\n"
                    for article_info in urls[:5]:
                        title = article_info.get('title', '')
                        url = article_info.get('url', '')
                        source = article_info.get('source', '')
                        if url:
                            result += f"- [{title}]({url}) - *{source}*\n"
                    result += "\n"
        
        return result
    except Exception as e:
        return f"âš ï¸ å ±å‘Šç”Ÿæˆå¤±æ•—: {e}"
