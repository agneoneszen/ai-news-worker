import feedparser
import datetime
import time
from dateutil import parser as date_parser

# RSS ä¾†æºæ¸…å–®
RSS_FEEDS = [
    {
        "source": "The Verge",
        "url": "https://www.theverge.com/rss/index.xml",
        "category": "Tech"
    },
    {
        "source": "CoinDesk",
        "url": "https://www.coindesk.com/arc/outboundfeeds/rss/",
        "category": "Crypto"
    },
    {
        "source": "TechCrunch",
        "url": "https://techcrunch.com/feed/",
        "category": "Tech"
    },
    {
        "source": "Ars Technica",
        "url": "https://feeds.arstechnica.com/arstechnica/index",
        "category": "Tech"
    },
    {
        "source": "Wired",
        "url": "https://www.wired.com/feed/rss",
        "category": "Tech"
    },
    {
        "source": "CoinTelegraph",
        "url": "https://cointelegraph.com/rss",
        "category": "Crypto"
    },
    {
        "source": "Decrypt",
        "url": "https://decrypt.co/feed",
        "category": "Crypto"
    },
    {
        "source": "The Block",
        "url": "https://www.theblock.co/rss.xml",
        "category": "Crypto"
    },
    {
        "source": "Hacker News",
        "url": "https://hnrss.org/frontpage",
        "category": "Tech"
    },
    {
        "source": "VentureBeat",
        "url": "https://venturebeat.com/feed/",
        "category": "Tech"
    }
]

def get_today_news():
    """
    ä¸»å‡½å¼ï¼šæŠ“å–éå»24å°æ™‚å…§çš„æ–°è (åš´æ ¼æ¨¡å¼)
    - æˆåŠŸï¼šå›å‚³æ–°èåˆ—è¡¨
    - å¤±æ•—ï¼šå›å‚³ç©ºåˆ—è¡¨ [] (çµ•å°ä¸å›å‚³å‡è³‡æ–™)
    """
    news_list = []
    print("ğŸ•·ï¸ [Scraper] é–‹å§‹æŠ“å–å¤–éƒ¨ RSSï¼ˆéå»24å°æ™‚ï¼‰...")
    
    # è¨ˆç®—24å°æ™‚å‰çš„æ™‚é–“
    now = datetime.datetime.now(datetime.timezone.utc)
    twenty_four_hours_ago = now - datetime.timedelta(hours=24)
    print(f"   ğŸ“… æ™‚é–“ç¯„åœ: {twenty_four_hours_ago.strftime('%Y-%m-%d %H:%M:%S UTC')} è‡³ç¾åœ¨")

    try:
        for feed_info in RSS_FEEDS:
            print(f"   - æ­£åœ¨è®€å–: {feed_info['source']}...")
            feed = feedparser.parse(feed_info['url'])
            
            if feed.bozo: # bozo=1 ä»£è¡¨è§£ææœ‰éŒ¯èª¤ (éæ¨™æº– XML æˆ–é€£ç·šå•é¡Œ)
                print(f"     âš ï¸ {feed_info['source']} è§£æè­¦å‘Š: {feed.bozo_exception}")
                continue

            # éæ­·æ‰€æœ‰æ–‡ç« ï¼Œéæ¿¾24å°æ™‚å…§çš„
            for entry in feed.entries:
                # è§£æç™¼å¸ƒæ™‚é–“
                try:
                    # feedparser æœƒè‡ªå‹•è§£ææ™‚é–“ï¼Œè½‰æ›ç‚º UTC
                    published_time = entry.get('published_parsed')
                    if published_time:
                        # è½‰æ›ç‚º datetime ç‰©ä»¶
                        published_dt = datetime.datetime(*published_time[:6], tzinfo=datetime.timezone.utc)
                    else:
                        # å¦‚æœæ²’æœ‰ published_parsedï¼Œå˜—è©¦è§£æ published å­—ä¸²
                        published_str = entry.get('published', '')
                        if published_str:
                            published_str = date_parser.parse(published_str)
                            if published_str.tzinfo is None:
                                published_str = published_str.replace(tzinfo=datetime.timezone.utc)
                        else:
                            # å¦‚æœå®Œå…¨æ²’æœ‰æ™‚é–“è³‡è¨Šï¼Œè·³é
                            continue
                    
                    # æª¢æŸ¥æ˜¯å¦åœ¨éå»24å°æ™‚å…§
                    if published_dt < twenty_four_hours_ago:
                        continue  # è¶…é24å°æ™‚ï¼Œè·³é
                        
                except Exception as e:
                    print(f"     âš ï¸ æ™‚é–“è§£æå¤±æ•—: {entry.get('title', 'Unknown')[:30]}... - {e}")
                    continue
                
                # æå–å…§å®¹
                content = ""
                if 'content' in entry:
                    content = entry.content[0].value
                elif 'summary' in entry:
                    content = entry.summary
                else:
                    content = entry.title

                # æ¸…ç† HTML
                import re
                clean_content = re.sub('<[^<]+?>', '', content)[:2000]  # å¢åŠ åˆ°2000å­—ä»¥ä¿ç•™æ›´å¤šå…§å®¹

                news_item = {
                    "title": entry.title,
                    "url": entry.link,
                    "content": clean_content,
                    "source": feed_info['source'],
                    "published_at": published_dt.isoformat(),
                    # é è¨­é¡åˆ¥ï¼Œç¨å¾Œ AI æœƒé‡æ–°åˆ†é¡
                    "category": feed_info['category'] 
                }
                news_list.append(news_item)
                
            # ç¦®è²Œæ€§å»¶é²ï¼Œé¿å…è¢«æ“‹
            time.sleep(1)
        
        print(f"âœ… [Scraper] æˆåŠŸæŠ“å– {len(news_list)} ç¯‡éå»24å°æ™‚å…§çš„çœŸå¯¦æ–°èã€‚")
        return news_list

    except Exception as e:
        print(f"âŒ [Scraper] ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        return [] # ç™¼ç”ŸéŒ¯èª¤ç›´æ¥å›å‚³ç©ºï¼Œä¸é€ å‡
