"""
é•·æœŸé‹è¡Œçš„æ’ç¨‹å™¨ç‰ˆæœ¬
é©ç”¨æ–¼ Railwayã€Render ç­‰éœ€è¦æŒçºŒé‹è¡Œçš„å¹³å°
"""
import schedule
import time
import os
import json
import datetime
import firebase_admin
from firebase_admin import credentials, firestore
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¼•å…¥æ¨¡çµ„
from ai_service import analyze_article, analyze_category_group, generate_daily_briefing
from scraper import get_today_news
from markdown_parser import parse_daily_briefing  # å¼•å…¥ Markdown è§£æå™¨
from collections import defaultdict

load_dotenv()

# ---------------------------------------------------------
# 1. åˆå§‹åŒ– Firebaseï¼ˆæ”¯æ´ç’°å¢ƒè®Šæ•¸æˆ–æª”æ¡ˆï¼‰
# ---------------------------------------------------------
if not firebase_admin._apps:
    # å„ªå…ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé©ç”¨æ–¼ Railway Secretsï¼‰
    if os.getenv("SERVICE_ACCOUNT_KEY"):
        try:
            cred_dict = json.loads(os.getenv("SERVICE_ACCOUNT_KEY"))
            cred = credentials.Certificate(cred_dict)
            print("âœ… å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥ Firebase æœå‹™å¸³è™Ÿé‡‘é‘°")
        except json.JSONDecodeError:
            print("âŒ ç’°å¢ƒè®Šæ•¸ SERVICE_ACCOUNT_KEY æ ¼å¼éŒ¯èª¤")
            raise
    else:
        # å¾æª”æ¡ˆè®€å–ï¼ˆæœ¬åœ°é–‹ç™¼æˆ–å‚³çµ±éƒ¨ç½²ï¼‰
        CRED_PATH = os.path.join(os.path.dirname(__file__), "serviceAccountKey.json")
        if os.path.exists(CRED_PATH):
            cred = credentials.Certificate(CRED_PATH)
            print(f"âœ… å¾æª”æ¡ˆè¼‰å…¥ Firebase æœå‹™å¸³è™Ÿé‡‘é‘°: {CRED_PATH}")
        else:
            raise FileNotFoundError(
                "æ‰¾ä¸åˆ° serviceAccountKey.json ä¸”æœªè¨­å®š SERVICE_ACCOUNT_KEY ç’°å¢ƒè®Šæ•¸"
            )
    
    firebase_admin.initialize_app(cred)

db = firestore.client()

# ---------------------------------------------------------
# 2. ä¸»æµç¨‹
# ---------------------------------------------------------
def job_pipeline():
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    print(f"ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥ä»»å‹™ï¼š{today_str}")

    # A. ç²å–åŸæ–™ (éå»24å°æ™‚çš„æ–°è)
    raw_news_list = get_today_news()

    # ğŸ”¥ é—œéµæª¢æŸ¥ï¼šå¦‚æœæ²’æœ‰æ–°èï¼Œç›´æ¥çµ‚æ­¢
    if not raw_news_list:
        print("âš ï¸ è­¦å‘Šï¼šéå»24å°æ™‚å…§ç„¡æ³•æŠ“å–åˆ°ä»»ä½•æ–°è (æˆ–ä¾†æºç¶²ç«™æ›äº†)ã€‚")
        print("ğŸ›‘ ä»»å‹™çµ‚æ­¢ï¼Œæœªå¯«å…¥ä»»ä½•è³‡æ–™ï¼Œä»¥ç¢ºä¿ç„¡å¹»è¦ºã€‚")
        return

    # B. å–®ç¯‡åˆ†é¡åˆ†æï¼ˆä¸¦è¡Œè™•ç†ä»¥æå‡æ•ˆç‡ï¼‰
    print(f"ğŸ§  [2/5] æ­£åœ¨åˆ†é¡ {len(raw_news_list)} ç¯‡æ–°èï¼ˆä¸¦è¡Œè™•ç†ï¼‰...")
    
    categorized_articles = defaultdict(list)
    
    # ä½¿ç”¨ä¸¦è¡Œè™•ç†åŠ é€Ÿåˆ†æï¼ˆæœ€å¤š 5 å€‹ä¸¦è¡Œï¼‰
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {
            executor.submit(
                analyze_article,
                text=news.get("content", ""),
                title=news.get("title", ""),
                source=news.get("source", ""),
                published_at=news.get("published_at", "")
            ): news
            for news in raw_news_list
        }
        
        completed = 0
        for future in as_completed(futures):
            news = futures[future]
            completed += 1
            try:
                analysis_result = future.result()
                
                if analysis_result:
                    # åˆä½µ AI åˆ†æçµæœ
                    processed_news = {**news, **analysis_result}
                    category = analysis_result.get('category', 'æœªåˆ†é¡')
                    categorized_articles[category].append(processed_news)
                    confidence = analysis_result.get('confidence', 0.0)
                    print(f"  [{completed}/{len(raw_news_list)}] å·²åˆ†é¡: {news['title'][:30]}... -> {category} (ä¿¡å¿ƒåº¦: {confidence:.2f})")
                else:
                    print(f"  [{completed}/{len(raw_news_list)}] åˆ†æå¤±æ•—è·³é: {news['title'][:30]}...")
            except Exception as e:
                print(f"  [{completed}/{len(raw_news_list)}] åˆ†æç•°å¸¸: {news['title'][:30]}... - {e}")

    if not categorized_articles:
        print("âŒ æ‰€æœ‰æ–°èåˆ†æçš†å¤±æ•—ï¼Œçµ‚æ­¢ä»»å‹™ã€‚")
        return

    # é™åˆ¶åˆ†é¡æ•¸é‡æœ€å¤š5å€‹ï¼ˆé¸æ“‡æ–‡ç« æ•¸æœ€å¤šçš„5å€‹åˆ†é¡ï¼‰
    if len(categorized_articles) > 5:
        print(f"âš ï¸ ç™¼ç¾ {len(categorized_articles)} å€‹åˆ†é¡ï¼Œå°‡ä¿ç•™æ–‡ç« æ•¸æœ€å¤šçš„5å€‹åˆ†é¡")
        sorted_categories = sorted(
            categorized_articles.items(), 
            key=lambda x: len(x[1]), 
            reverse=True
        )
        categorized_articles = dict(sorted_categories[:5])
        print(f"âœ… ä¿ç•™åˆ†é¡: {', '.join(categorized_articles.keys())}")

    # C. çµ±åˆåŒé¡æ–‡ç« ä¸¦åˆ†æ
    print(f"ğŸ“š [3/5] æ­£åœ¨çµ±åˆä¸¦åˆ†æ {len(categorized_articles)} å€‹åˆ†é¡...")
    category_analyses = []
    
    for category, articles in categorized_articles.items():
        print(f"  - åˆ†æåˆ†é¡ã€Œ{category}ã€({len(articles)}ç¯‡)...")
        category_analysis = analyze_category_group(category, articles)
        if category_analysis:
            category_analyses.append(category_analysis)
            print(f"    âœ… å®Œæˆ")
        else:
            print(f"    âŒ åˆ†æå¤±æ•—")

    if not category_analyses:
        print("âŒ æ‰€æœ‰åˆ†é¡åˆ†æçš†å¤±æ•—ï¼Œçµ‚æ­¢ä»»å‹™ã€‚")
        return

    # D. ç”Ÿæˆæ¯æ—¥æ±ºç­–æ—¥å ±ï¼ˆå‚³éåŸå§‹æ–‡ç« åˆ—è¡¨ä»¥åŒ…å«ä¾†æºé€£çµï¼‰
    print("ğŸ“ [4/5] æ­£åœ¨æ’°å¯«æ¯æ—¥æ±ºç­–æ—¥å ±...")
    # æ”¶é›†æ‰€æœ‰å·²è™•ç†çš„æ–‡ç« ï¼ˆåŒ…å« URLï¼‰
    all_processed_articles = []
    for articles in categorized_articles.values():
        all_processed_articles.extend(articles)
    daily_briefing_md = generate_daily_briefing(category_analyses, source_articles=all_processed_articles)

    # E. å¯«å…¥ Firestore
    print("ğŸ’¾ [5/5] æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
    try:
        # è¨ˆç®—ç¸½æ–‡ç« æ•¸
        total_articles = sum(len(articles) for articles in categorized_articles.values())
        
        # æº–å‚™åˆ†é¡æ‘˜è¦è³‡æ–™ï¼ˆä½¿ç”¨å„ªåŒ–ç‰ˆçµæ§‹ï¼‰
        category_summaries = []
        for cat_analysis in category_analyses:
            category_summaries.append({
                'category': cat_analysis.get('category'),
                'article_count': cat_analysis.get('article_count', 0),
                'executive_summary': cat_analysis.get('executive_summary', cat_analysis.get('summary', '')),
                'storylines': cat_analysis.get('storylines', []),
                'key_points': cat_analysis.get('key_points', []),
                'risks': cat_analysis.get('risks', []),
                'opportunities': cat_analysis.get('opportunities', []),
                'signals_to_watch': cat_analysis.get('signals_to_watch', []),
                'confidence': cat_analysis.get('confidence', 0.0)
            })
        
        # è§£æ Markdown å…§å®¹ç‚ºçµæ§‹åŒ–æ ¼å¼
        print("ğŸ“Š æ­£åœ¨è§£æ Markdown å…§å®¹ç‚ºçµæ§‹åŒ–æ ¼å¼...")
        structured_content = parse_daily_briefing(daily_briefing_md)
        
        db.collection('daily_news').document(today_str).set({
            'date_str': today_str,
            'content': daily_briefing_md,  # ä¿ç•™åŸå§‹ Markdownï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            'structured': structured_content,  # æ–°å¢ï¼šçµæ§‹åŒ–å…§å®¹
            'article_count': total_articles,
            'category_count': len(category_analyses),
            'categories': [cat.get('category') for cat in category_analyses],
            'category_summaries': category_summaries,  # æ–°å¢ï¼šåˆ†é¡æ‘˜è¦
            'created_at': firestore.SERVER_TIMESTAMP,
            'status': 'published'
        })
        
        print(f"âœ… ä»»å‹™æˆåŠŸï¼çœŸå¯¦æ—¥å ±å·²å­˜å…¥: daily_news/{today_str}")
        print(f"   ğŸ“Š çµ±è¨ˆ: {total_articles} ç¯‡æ–‡ç« ï¼Œ{len(category_analyses)} å€‹åˆ†é¡")

    except Exception as e:
        print(f"âŒ Firestore å¯«å…¥éŒ¯èª¤: {e}")

# ---------------------------------------------------------
# 3. æ’ç¨‹è¨­å®šï¼ˆæ¸¬è©¦æ¨¡å¼ï¼šæš«åœè‡ªå‹•æ’ç¨‹ï¼‰
# ---------------------------------------------------------
# âš ï¸ æ¸¬è©¦æ¨¡å¼ï¼šå·²æš«åœè‡ªå‹•æ’ç¨‹
# schedule.every().day.at("09:00").do(job_pipeline)

# ç«‹å³åŸ·è¡Œä¸€æ¬¡ï¼ˆæ¸¬è©¦ç”¨ï¼‰
print("ğŸ§ª [æ¸¬è©¦æ¨¡å¼] ç«‹å³åŸ·è¡Œä¸€æ¬¡æ¸¬è©¦...")
print("âš ï¸  æ³¨æ„ï¼šè‡ªå‹•æ’ç¨‹å·²æš«åœï¼Œåƒ…åŸ·è¡Œä¸€æ¬¡")
job_pipeline()

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª AI News Worker æ’ç¨‹å™¨ï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰")
    print("=" * 60)
    print("âš ï¸  è‡ªå‹•æ’ç¨‹å·²æš«åœ")
    print("âœ… å·²åŸ·è¡Œä¸€æ¬¡æ¸¬è©¦ä»»å‹™")
    print("=" * 60)
    print("")
    print("ğŸ’¡ æ¸¬è©¦å®Œæˆå¾Œï¼Œè«‹å–æ¶ˆè¨»è§£ schedule ç›¸é—œä»£ç¢¼ä»¥å•Ÿç”¨è‡ªå‹•æ’ç¨‹")
    print("")
    
    # æ¸¬è©¦æ¨¡å¼ï¼šä¸é€²å…¥ä¸»å¾ªç’°ï¼ŒåŸ·è¡Œå®Œç•¢å¾Œé€€å‡º
    # å¦‚æœéœ€è¦æŒçºŒé‹è¡Œï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„è¨»è§£
    # while True:
    #     schedule.run_pending()
    #     time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
    
    print("âœ… æ¸¬è©¦åŸ·è¡Œå®Œæˆï¼Œç¨‹åºçµæŸ")

