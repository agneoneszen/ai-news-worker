"""
LLM çµæœå¿«å–ç®¡ç†å™¨
é¿å…å°ç›¸åŒå…§å®¹é‡è¤‡å‘¼å« LLM API
"""

import os
import json
import hashlib
from pathlib import Path
from datetime import datetime, timedelta

# å¿«å–ç›®éŒ„
CACHE_DIR = Path(__file__).parent / ".llm_cache"
CACHE_DIR.mkdir(exist_ok=True)

# å¿«å–éæœŸæ™‚é–“ï¼ˆå¤©ï¼‰
CACHE_EXPIRY_DAYS = 7

def get_content_hash(content, metadata=None):
    """
    ç”Ÿæˆå…§å®¹çš„é›œæ¹Šå€¼
    
    Args:
        content: ä¸»è¦å…§å®¹ï¼ˆæ–°èå…§å®¹ï¼‰
        metadata: å¯é¸çš„ metadataï¼ˆtitle, source ç­‰ï¼‰
    
    Returns:
        str: å…§å®¹çš„ MD5 é›œæ¹Šå€¼
    """
    # çµ„åˆå…§å®¹å’Œ metadata
    combined = content[:2000]  # åªå–å‰2000å­—ä½œç‚ºé—œéµéƒ¨åˆ†
    
    if metadata:
        # åŠ å…¥ metadata ä½†ä¸åŒ…æ‹¬æ™‚é–“ï¼ˆæ™‚é–“ä¸å½±éŸ¿å…§å®¹åˆ†æï¼‰
        if isinstance(metadata, dict):
            combined += str(metadata.get('title', ''))
            combined += str(metadata.get('source', ''))
    
    # ç”Ÿæˆ MD5 é›œæ¹Š
    return hashlib.md5(combined.encode('utf-8')).hexdigest()

def get_cache_path(cache_key, cache_type="article"):
    """
    å–å¾—å¿«å–æª”æ¡ˆè·¯å¾‘
    
    Args:
        cache_key: å¿«å–éµå€¼ï¼ˆé›œæ¹Šå€¼ï¼‰
        cache_type: å¿«å–é¡å‹ï¼ˆarticle, category, briefingï¼‰
    
    Returns:
        Path: å¿«å–æª”æ¡ˆè·¯å¾‘
    """
    return CACHE_DIR / f"{cache_type}_{cache_key}.json"

def is_cache_valid(cache_path):
    """
    æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆï¼ˆæœªéæœŸï¼‰
    
    Args:
        cache_path: å¿«å–æª”æ¡ˆè·¯å¾‘
    
    Returns:
        bool: å¿«å–æ˜¯å¦æœ‰æ•ˆ
    """
    if not cache_path.exists():
        return False
    
    # æª¢æŸ¥æª”æ¡ˆä¿®æ”¹æ™‚é–“
    mtime = datetime.fromtimestamp(cache_path.stat().st_mtime)
    age = datetime.now() - mtime
    
    return age < timedelta(days=CACHE_EXPIRY_DAYS)

def get_cached_result(cache_key, cache_type="article"):
    """
    å¾å¿«å–å–å¾—çµæœ
    
    Args:
        cache_key: å¿«å–éµå€¼
        cache_type: å¿«å–é¡å‹
    
    Returns:
        dict: å¿«å–çš„çµæœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–éæœŸå‰‡è¿”å› None
    """
    cache_path = get_cache_path(cache_key, cache_type)
    
    if not is_cache_valid(cache_path):
        return None
    
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"ğŸ’¾ [Cache] ä½¿ç”¨å¿«å–çµæœ: {cache_type} ({cache_key[:8]}...)")
            return data.get('result')
    except Exception as e:
        print(f"âš ï¸ [Cache] è®€å–å¿«å–å¤±æ•—: {e}")
        return None

def save_cached_result(cache_key, result, cache_type="article", metadata=None):
    """
    å„²å­˜çµæœåˆ°å¿«å–
    
    Args:
        cache_key: å¿«å–éµå€¼
        result: è¦å¿«å–çš„çµæœ
        cache_type: å¿«å–é¡å‹
        metadata: å¯é¸çš„ metadataï¼ˆç”¨æ–¼è¨˜éŒ„ï¼‰
    """
    cache_path = get_cache_path(cache_key, cache_type)
    
    try:
        data = {
            'result': result,
            'cached_at': datetime.now().isoformat(),
            'cache_type': cache_type,
            'metadata': metadata or {}
        }
        
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ [Cache] å·²å„²å­˜å¿«å–: {cache_type} ({cache_key[:8]}...)")
    except Exception as e:
        print(f"âš ï¸ [Cache] å„²å­˜å¿«å–å¤±æ•—: {e}")

def clear_expired_cache():
    """
    æ¸…é™¤éæœŸçš„å¿«å–æª”æ¡ˆ
    """
    if not CACHE_DIR.exists():
        return
    
    expired_count = 0
    for cache_file in CACHE_DIR.glob("*.json"):
        if not is_cache_valid(cache_file):
            try:
                cache_file.unlink()
                expired_count += 1
            except Exception as e:
                print(f"âš ï¸ [Cache] åˆªé™¤éæœŸå¿«å–å¤±æ•—: {e}")
    
    if expired_count > 0:
        print(f"ğŸ—‘ï¸ [Cache] å·²æ¸…é™¤ {expired_count} å€‹éæœŸå¿«å–æª”æ¡ˆ")

def get_cache_stats():
    """
    å–å¾—å¿«å–çµ±è¨ˆè³‡è¨Š
    
    Returns:
        dict: å¿«å–çµ±è¨ˆè³‡è¨Š
    """
    if not CACHE_DIR.exists():
        return {'total': 0, 'by_type': {}}
    
    stats = {'total': 0, 'by_type': {}}
    
    for cache_file in CACHE_DIR.glob("*.json"):
        stats['total'] += 1
        cache_type = cache_file.stem.split('_')[0]
        stats['by_type'][cache_type] = stats['by_type'].get(cache_type, 0) + 1
    
    return stats

